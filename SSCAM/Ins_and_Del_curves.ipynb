{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbMZQ/LwikfwhHWZOxtnOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0cketr1kky/SSCAM_supplements/blob/master/Ins_and_Del_curves.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDkKvnDr94rl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2c9f4a47-fbd9-4979-d6c6-f428f6a164bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGXK3L7E98p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.cm as mpl_color_map\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def convert_to_grayscale(im_as_arr):\n",
        "    \"\"\"\n",
        "        Converts 3d image to grayscale\n",
        "    Args:\n",
        "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
        "    returns:\n",
        "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
        "    \"\"\"\n",
        "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "    return grayscale_im\n",
        "\n",
        "\n",
        "def save_gradient_images(gradient, file_name):\n",
        "    \"\"\"\n",
        "        Exports the original gradient image\n",
        "    Args:\n",
        "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
        "        file_name (str): File name to be exported\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Normalize\n",
        "    gradient = gradient - gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    # Save image\n",
        "    path_to_file = os.path.join('../results', file_name + '.jpg')\n",
        "    save_image1(gradient, path_to_file)\n",
        "\n",
        "\n",
        "def save_class_activation_images(org_img, activation_map):\n",
        "    \"\"\"\n",
        "        Saves cam activation map and activation map on the original image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    # Save colored heatmap\n",
        "    path_to_file = os.path.join('../results', '/_Cam_Heatmap.png')\n",
        "    save_image1(heatmap, path_to_file)\n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join('../results', '/_Cam_On_Image.png')\n",
        "    save_image1(heatmap_on_image, path_to_file)\n",
        "    # SAve grayscale heatmap\n",
        "    path_to_file = os.path.join('../results', '/_Cam_Grayscale.png')\n",
        "    save_image1(activation_map, path_to_file)\n",
        "\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "        Apply heatmap on image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        colormap_name (str): Name of the colormap\n",
        "    \"\"\"\n",
        "    # Get colormap\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    # Change alpha channel in colormap to make sure original image is displayed\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    # Apply heatmap on iamge\n",
        "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n",
        "\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "\n",
        "def save_image1(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    #ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im\n",
        "\n",
        "\n",
        "def get_positive_negative_saliency(gradient):\n",
        "    \"\"\"\n",
        "        Generates positive and negative saliency maps based on the gradient\n",
        "    Args:\n",
        "        gradient (numpy arr): Gradient of the operation to visualize\n",
        "    returns:\n",
        "        pos_saliency ( )\n",
        "    \"\"\"\n",
        "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
        "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
        "    return pos_saliency, neg_saliency\n",
        "\n",
        "\n",
        "def get_example_params(example_index):\n",
        "    \"\"\"\n",
        "        Gets used variables for almost all visualizations, like the image, model etc.\n",
        "    Args:\n",
        "        example_index (int): Image id to use from examples\n",
        "    returns:\n",
        "        original_image (numpy arr): Original image read from the file\n",
        "        prep_img (numpy_arr): Processed image\n",
        "        target_class (int): Target class for the image\n",
        "        pretrained_model(Pytorch model): Model to use for the operations\n",
        "    \"\"\"\n",
        "    # Pick one of the examples\n",
        "    \n",
        "    # Read image\n",
        "    original_image = Image.open(img_path).convert('RGB')\n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    # Define model\n",
        "    #pretrained_model = model\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class)\n",
        "    \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseProp(object):\n",
        "    \"\"\"\n",
        "        Base class for backpropagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        \"\"\"Init\n",
        "        # Arguments:\n",
        "            model: torchvision.models. A pretrained model.\n",
        "            handle: list. Handle list that register a hook function.\n",
        "            relu_outputs: list. Forward output after relu.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.handle = []\n",
        "        self.relu_outputs = []\n",
        "\n",
        "    def _register_conv_hook(self):\n",
        "\n",
        "        \"\"\"\n",
        "            Register hook function to save gradient w.r.t input image.\n",
        "        \"\"\"\n",
        "\n",
        "        def _record_gradients(module, grad_in, grad_out):\n",
        "                self.gradients = grad_in[0]\n",
        "\n",
        "        for _, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.modules.conv.Conv2d) and module.in_channels == 3:\n",
        "                backward_handle = module.register_backward_hook(_record_gradients)\n",
        "                self.handle.append(backward_handle)\n",
        "\n",
        "    def _register_relu_hooks(self):\n",
        "\n",
        "        \"\"\"\n",
        "            Register hook function to save forward and backward relu result.\n",
        "        \"\"\"\n",
        "\n",
        "        # Save forward propagation output of the ReLU layer\n",
        "        def _record_output(module, input_, output):\n",
        "            self.relu_outputs.append(output)\n",
        "\n",
        "        def _clip_gradients(module, grad_in, grad_out):\n",
        "            # keep positive forward propagation output\n",
        "            relu_output = self.relu_outputs.pop()\n",
        "            relu_output[relu_output > 0] = 1\n",
        "\n",
        "            # keep positive backward propagation gradient\n",
        "            positive_grad_out = torch.clamp(grad_out[0], min=0.0)\n",
        "\n",
        "            # generate modified guided gradient\n",
        "            modified_grad_out = positive_grad_out * relu_output\n",
        "\n",
        "            return (modified_grad_out, )\n",
        "\n",
        "        for _, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.ReLU):\n",
        "                forward_handle = module.register_forward_hook(_record_output)\n",
        "                backward_handle = module.register_backward_hook(_clip_gradients)\n",
        "                self.handle.append(forward_handle)\n",
        "                self.handle.append(backward_handle)\n",
        "\n",
        "class VanillaBackprop():\n",
        "    \"\"\"\n",
        "        Produces gradients generated with vanilla back propagation from the image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        # Hook the first layer to get the gradient\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[0]\n",
        "\n",
        "        # Register hook to the first layer\n",
        "        first_layer = list(self.model.features._modules.items())[0][1]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero grads\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
        "        return gradients_as_arr\n",
        "\n",
        "class Backprop(BaseProp):\n",
        "\n",
        "    \"\"\" Generates vanilla or guided backprop gradients of a target class output w.r.t. an input image.\n",
        "        # Arguments:\n",
        "            model: torchvision.models. A pretrained model.\n",
        "            guided: bool. If True, perform guided backpropagation. Defaults to False.\n",
        "        # Return:\n",
        "            Backprop Class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, guided=False):\n",
        "        super().__init__(model)\n",
        "        self.model.eval()\n",
        "        self.guided = guided\n",
        "        self.gradients = None\n",
        "        self._register_conv_hook()\n",
        "\n",
        "    def calculate_gradients(self,\n",
        "                            input_,\n",
        "                            target_class=None,\n",
        "                            take_max=False,\n",
        "                            use_gpu=False):\n",
        "\n",
        "        \"\"\" Calculate gradient.\n",
        "            # Arguments\n",
        "                input_: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "                target_class: int. Index of target class. Default to None and use the prediction result as target class.\n",
        "                take_max: bool. Take the maximum across colour channels. Defaults to False.\n",
        "                use_gpu. bool. Use GPU or not. Defaults to False.\n",
        "            # Return:\n",
        "                Gradient (torch.Tensor) with shape (C, H, W). If take max is True, with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        if self.guided:\n",
        "            self._register_relu_hooks()\n",
        "\n",
        "        # Create a empty tensor to save gradients\n",
        "        self.gradients = torch.zeros(input_.shape)\n",
        "\n",
        "        output = self.model(input_)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        if output.shape == torch.Size([1]):\n",
        "            target = None\n",
        "        else:\n",
        "            pred_class = output.argmax().item()\n",
        "\n",
        "            # Create a Tensor with zero elements, set the element at pred class index to be 1\n",
        "            target = torch.zeros(output.shape)\n",
        "\n",
        "            # If target class is None, calculate gradient of predicted class.\n",
        "            if target_class is None:\n",
        "                target[0][pred_class] = 1\n",
        "            else:\n",
        "                target[0][target_class] = 1\n",
        "\n",
        "            \n",
        "        # Calculate gradients w.r.t. input image\n",
        "        output.backward(gradient=target)\n",
        "\n",
        "        gradients = self.gradients.detach().cpu()[0]\n",
        "\n",
        "        if take_max:\n",
        "            gradients = gradients.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "        for module in self.handle:\n",
        "            module.remove()\n",
        "        gradients = gradients.numpy()\n",
        "        return gradients\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# from guided_backprop import GuidedBackprop  # To use with guided backprop\n",
        "\n",
        "\n",
        "def generate_smooth_grad(Backprop, prep_img, target_class, param_n, param_sigma_multiplier):\n",
        "    \"\"\"\n",
        "        Generates smooth gradients of given Backprop type. You can use this with both vanilla\n",
        "        and guided backprop\n",
        "    Args:\n",
        "        Backprop (class): Backprop type\n",
        "        prep_img (torch Variable): preprocessed image\n",
        "        target_class (int): target class of imagenet\n",
        "        param_n (int): Amount of images used to smooth gradient\n",
        "        param_sigma_multiplier (int): Sigma multiplier when calculating std of noise\n",
        "    \"\"\"\n",
        "    # Generate an empty image/matrix\n",
        "    smooth_grad = np.zeros(prep_img.size()[1:])\n",
        "\n",
        "    mean = 0\n",
        "    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()\n",
        "    \n",
        "    for x in range(param_n):\n",
        "        # Generate noise\n",
        "        noise = Variable(prep_img.data.new(prep_img.size()).normal_(mean, sigma**2))\n",
        "        # Add noise to the image\n",
        "        noisy_img = prep_img + noise\n",
        "        # Calculate gradients\n",
        "        vanilla_grads = Backprop.calculate_gradients(noisy_img, target_class)\n",
        "        # Add gradients to smooth_grad\n",
        "        smooth_grad = smooth_grad + vanilla_grads\n",
        "    # Average it out\n",
        "    smooth_grad = smooth_grad / param_n\n",
        "    return smooth_grad\n",
        "\n",
        "def process_image(image_path):\n",
        "    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    # Resize\n",
        "    img = image.resize((256, 256))\n",
        "\n",
        "    # Center crop\n",
        "    width = 256\n",
        "    height = 256\n",
        "    new_width = 224\n",
        "    new_height = 224\n",
        "\n",
        "    left = (width - new_width) / 2\n",
        "    top = (height - new_height) / 2\n",
        "    right = (width + new_width) / 2\n",
        "    bottom = (height + new_height) / 2\n",
        "    img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # Convert to numpy, transpose color dimension and normalize\n",
        "    img = np.array(img).transpose((2, 0, 1)) / 256\n",
        "\n",
        "    # Standardization\n",
        "    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
        "    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
        "\n",
        "    img = img - means\n",
        "    img = img / stds\n",
        "\n",
        "    img_tensor = torch.Tensor(img)\n",
        "\n",
        "    return img_tensor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghil0QMQ-Ofz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"flashtorch.utils\n",
        "\n",
        "This module provides utility functions for image handling and tensor\n",
        "transformation.\n",
        "\n",
        "\"\"\"\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.models\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"Loads image as a PIL RGB image.\n",
        "\n",
        "        Args:\n",
        "            - **image_path (str) - **: A path to the image\n",
        "\n",
        "        Returns:\n",
        "            An instance of PIL.Image.Image in RGB\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return Image.open(image_path).convert('L')\n",
        "\n",
        "\n",
        "def load_image1(image_path):\n",
        "    \"\"\"Loads image as a PIL RGB image.\n",
        "\n",
        "        Args:\n",
        "            - **image_path (str) - **: A path to the image\n",
        "\n",
        "        Returns:\n",
        "            An instance of PIL.Image.Image in RGB\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return Image.open(image_path).convert('RGB')\n",
        "\n",
        "\n",
        "def apply_transforms(image, size=95):\n",
        "    \"\"\"Transforms a PIL image to torch.Tensor.\n",
        "\n",
        "    Applies a series of tranformations on PIL image including a conversion\n",
        "    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n",
        "    is ready to be used as an input to neural networks.\n",
        "\n",
        "    First the image is resized to 256, then cropped to 224. The `means` and\n",
        "    `stds` for normalisation are taken from numbers used in ImageNet, as\n",
        "    currently developing the package for visualizing pre-trained models.\n",
        "\n",
        "    The plan is to to expand this to handle custom size/mean/std.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy array)\n",
        "        size (int, optional, default=224): Desired size (width/height) of the\n",
        "            output tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)` for numpy array\n",
        "        Output: :math:`(N, C, H, W)`\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor (torch.float32): Transformed image tensor\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds)\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    tensor.requires_grad = True\n",
        "\n",
        "    return tensor\n",
        "\n",
        "def apply_transforms_v0(image, size=224):\n",
        "    \"\"\"Transforms a PIL image to torch.Tensor.\n",
        "\n",
        "    Applies a series of tranformations on PIL image including a conversion\n",
        "    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n",
        "    is ready to be used as an input to neural networks.\n",
        "\n",
        "    First the image is resized to 256, then cropped to 224. The `means` and\n",
        "    `stds` for normalisation are taken from numbers used in ImageNet, as\n",
        "    currently developing the package for visualizing pre-trained models.\n",
        "\n",
        "    The plan is to to expand this to handle custom size/mean/std.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy array)\n",
        "        size (int, optional, default=224): Desired size (width/height) of the\n",
        "            output tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)` for numpy array\n",
        "        Output: :math:`(N, C, H, W)`\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor (torch.float32): Transformed image tensor\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    tensor.requires_grad = True\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Reverses the normalisation on a tensor.\n",
        "\n",
        "    Performs a reverse operation on a tensor, so the pixel value range is\n",
        "    between 0 and 1. Useful for when plotting a tensor into an image.\n",
        "\n",
        "    Normalisation: (image - mean) / std\n",
        "    Denormalisation: image * std + mean\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(N, C, H, W)`\n",
        "        Output: :math:`(N, C, H, W)` (same shape as input)\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n",
        "            values between [0, 1]\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    denormalized = tensor.clone()\n",
        "\n",
        "    for channel, mean, std in zip(denormalized[0], means, stds):\n",
        "        channel.mul_(std).add_(mean)\n",
        "\n",
        "    return denormalized\n",
        "\n",
        "\n",
        "def standardize_and_clip(tensor, min_value=0.0, max_value=1.0):\n",
        "    \"\"\"Standardizes and clips input tensor.\n",
        "\n",
        "    Standardize the input tensor (mean = 0.0, std = 1.0), ensures std is 0.1\n",
        "    and clips it to values between min/max (default: 0.0/1.0).\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor):\n",
        "        min_value (float, optional, default=0.0)\n",
        "        max_value (float, optional, default=1.0)\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)`\n",
        "        Output: Same as the input\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Normalised tensor with values between\n",
        "            [min_value, max_value]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    tensor = tensor.detach().cpu()\n",
        "\n",
        "    mean = tensor.mean()\n",
        "    std = tensor.std()\n",
        "    if std == 0:\n",
        "        std += 1e-7\n",
        "\n",
        "    standardized = tensor.sub(mean).div(std).mul(0.1)\n",
        "    clipped = standardized.add(0.5).clamp(min_value, max_value)\n",
        "\n",
        "    return clipped\n",
        "\n",
        "\n",
        "def format_for_plotting(tensor):\n",
        "    \"\"\"Formats the shape of tensor for plotting.\n",
        "\n",
        "    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
        "    which is not suitable for plotting as images. This function formats an\n",
        "    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n",
        "    data.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor, torch.float32): Image tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
        "        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Formatted image tensor (detached)\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    has_batch_dimension = len(tensor.shape) == 4\n",
        "    formatted = tensor.clone()\n",
        "\n",
        "    if has_batch_dimension:\n",
        "        formatted = tensor.squeeze(0)\n",
        "\n",
        "    if formatted.shape[0] == 1:\n",
        "        return formatted.squeeze(0).detach()\n",
        "    else:\n",
        "        return formatted.permute(1, 2, 0).detach()\n",
        "\n",
        "\n",
        "def visualize(input_, gradients, save_path=None, cmap='viridis', alpha=0.7):\n",
        "\n",
        "    \"\"\" Method to plot the explanation.\n",
        "\n",
        "        # Arguments\n",
        "            input_: Tensor. Original image.\n",
        "            gradients: Tensor. Saliency map result.\n",
        "            save_path: String. Defaults to None.\n",
        "            cmap: Defaults to be 'viridis'.\n",
        "            alpha: Defaults to be 0.7.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    input_ = format_for_plotting(denormalize(input_))\n",
        "    gradients = format_for_plotting(standardize_and_clip(gradients))\n",
        "\n",
        "    subplots = [\n",
        "        ('', [(input_, None, None)]),\n",
        "        ('', [(gradients, None, None)]),\n",
        "        ('', [(input_, None, None), (gradients, cmap, alpha)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 3))\n",
        "\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        for image, cmap, alpha in images:\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "\n",
        "        ax.set_title(title)\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def basic_visualize(input_, gradients1, gradients2, gradients3, gradients4, \\\n",
        "                    gradients5, gradients6, save_path=None, weight=None, cmap='viridis', alpha=0.7):\n",
        "\n",
        "    \"\"\" Method to plot the explanation.\n",
        "\n",
        "        # Arguments\n",
        "            input_: Tensor. Original image.\n",
        "            gradients: Tensor. Saliency map result.\n",
        "            save_path: String. Defaults to None.\n",
        "            cmap: Defaults to be 'viridis'.\n",
        "            alpha: Defaults to be 0.7.\n",
        "\n",
        "    \"\"\"\n",
        "    input_ = format_for_plotting(denormalize(input_))\n",
        "    gradients1 = format_for_plotting(standardize_and_clip(gradients1))\n",
        "    gradients2 = format_for_plotting(standardize_and_clip(gradients2))\n",
        "    gradients3 = format_for_plotting(standardize_and_clip(gradients3))\n",
        "    gradients4 = format_for_plotting(standardize_and_clip(gradients4))\n",
        "    gradients5 = format_for_plotting(standardize_and_clip(gradients5))\n",
        "    gradients6 = format_for_plotting(standardize_and_clip(gradients6))\n",
        "\n",
        "    subplots = [\n",
        "        ('', [(input_, None, None)]),\n",
        "        ('', [(input_, None, None), (gradients1, cmap, alpha)]), \\\n",
        "        ('', [(input_, None, None), (gradients2, cmap, alpha)]), \\\n",
        "        ('', [(input_, None, None), (gradients3, cmap, alpha)]), \\\n",
        "        ('', [(input_, None, None), (gradients4, cmap, alpha)]), \\\n",
        "        ('', [(input_, None, None), (gradients5, cmap, alpha)]), \\\n",
        "        ('', [(input_, None, None), (gradients6, cmap, alpha)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 3))\n",
        "    count=0\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        for image, cmap, alpha in images:\n",
        "            print('hello {}'.format(count))\n",
        "            count+=1\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def find_resnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'conv1'\n",
        "            target_layer_name = 'layer1'\n",
        "            target_layer_name = 'layer1_basicblock0'\n",
        "            target_layer_name = 'layer1_basicblock0_relu'\n",
        "            target_layer_name = 'layer1_bottleneck0'\n",
        "            target_layer_name = 'layer1_bottleneck0_conv1'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample_0'\n",
        "            target_layer_name = 'avgpool'\n",
        "            target_layer_name = 'fc'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'layer4'\n",
        "\n",
        "    if 'layer' in target_layer_name:\n",
        "        hierarchy = target_layer_name.split('_')\n",
        "        layer_num = int(hierarchy[0].lstrip('layer'))\n",
        "        if layer_num == 1:\n",
        "            target_layer = arch.layer1\n",
        "        elif layer_num == 2:\n",
        "            target_layer = arch.layer2\n",
        "        elif layer_num == 3:\n",
        "            target_layer = arch.layer3\n",
        "        elif layer_num == 4:\n",
        "            target_layer = arch.layer4\n",
        "        else:\n",
        "            raise ValueError('unknown layer : {}'.format(target_layer_name))\n",
        "\n",
        "        if len(hierarchy) >= 2:\n",
        "            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n",
        "            target_layer = target_layer[bottleneck_num]\n",
        "\n",
        "        if len(hierarchy) >= 3:\n",
        "            target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "        if len(hierarchy) == 4:\n",
        "            target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    else:\n",
        "        target_layer = arch._modules[target_layer_name]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_densenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_transition1'\n",
        "            target_layer_name = 'features_transition1_norm'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'classifier'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) >= 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    if len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_vgg_layer(arch, target_layer_name):\n",
        "    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_42'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_alexnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "              saliency_map2 = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_0'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features_29'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_squeezenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision densenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'features_12'\n",
        "                target_layer_name = 'features_12_expand3x3'\n",
        "                target_layer_name = 'features_12_expand3x3_activation'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_googlenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'inception5b'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_mobilenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'features'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_shufflenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'conv5'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_layer(arch, target_layer_name):\n",
        "    \"\"\"Find target layer to calculate CAM.\n",
        "\n",
        "        : Args:\n",
        "            - **arch - **: Self-defined architecture.\n",
        "            - **target_layer_name - ** (str): Name of target class.\n",
        "\n",
        "        : Return:\n",
        "            - **target_layer - **: Found layer. This layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    if target_layer_name.split('_') not in arch._modules.keys():\n",
        "        raise Exception(\"Invalid target layer name.\")\n",
        "    target_layer = arch._modules[target_layer_name]\n",
        "    return target_layer\n",
        "'''\n",
        "Part of code borrows from https://github.com/1Konny/gradcam_plus_plus-pytorch\n",
        "'''\n",
        "\n",
        "import torch\n",
        "\n",
        "class BaseCAM(object):\n",
        "    \"\"\" Base class for Class activation mapping.\n",
        "        : Args\n",
        "            - **model_dict -** : Dict. Has format as dict(type='vgg', arch=torchvision.models.vgg16(pretrained=True),\n",
        "            layer_name='features',input_size=(224, 224)).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        model_type = model_dict['type']\n",
        "        layer_name = model_dict['layer_name']\n",
        "        \n",
        "        self.model_arch = model_dict['arch']\n",
        "        self.model_arch.eval()\n",
        "        #if torch.cuda.is_available():\n",
        "        #  self.model_arch.cuda()\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            #if torch.cuda.is_available():\n",
        "            #  self.gradients['value'] = grad_output[0].cuda()\n",
        "            \n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            \n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            #if torch.cuda.is_available():\n",
        "            #  self.activations['value'] = output.cuda()\n",
        "            self.activations['value'] = output\n",
        "            \n",
        "            return None\n",
        "\n",
        "        if 'vgg' in model_type.lower():\n",
        "            self.target_layer = find_vgg_layer(self.model_arch, layer_name)\n",
        "        elif 'resnet' in model_type.lower():\n",
        "            self.target_layer = find_resnet_layer(self.model_arch, layer_name)\n",
        "        elif 'densenet' in model_type.lower():\n",
        "            self.target_layer = find_densenet_layer(self.model_arch, layer_name)\n",
        "        elif 'alexnet' in model_type.lower():\n",
        "            self.target_layer = find_alexnet_layer(self.model_arch, layer_name)\n",
        "        elif 'squeezenet' in model_type.lower():\n",
        "            self.target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n",
        "        elif 'googlenet' in model_type.lower():\n",
        "            self.target_layer = find_googlenet_layer(self.model_arch, layer_name)\n",
        "        elif 'shufflenet' in model_type.lower():\n",
        "            self.target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n",
        "        elif 'mobilenet' in model_type.lower():\n",
        "            self.target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n",
        "        else:\n",
        "            self.target_layer = find_layer(self.model_arch, layer_name)\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        return None\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GradCAM(BaseCAM):\n",
        "    \"\"\"\n",
        "        GradCAM, inherit from BaseCAM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input_, class_idx=None, retain_graph=False):\n",
        "        \"\"\"Generates GradCAM result.\n",
        "\n",
        "        # Arguments\n",
        "            input_: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "            class_idx: int. Index of target class. Defaults to be index of predicted class.\n",
        "\n",
        "        # Return\n",
        "            Result of GradCAM (torch.Tensor) with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        b, c, h, w = input_.size()\n",
        "        logit = self.model_arch(input_)\n",
        "\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        \n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SmoothGradCAMpp(GradCAM):\n",
        "  \n",
        "  def __init__(self, model_dict):\n",
        "    super(SmoothGradCAMpp, self).__init__(model_dict)\n",
        "\n",
        "  def forward(self, input_image, class_idx=None, retain_graph=False):\n",
        "\n",
        "    b, c, h, w = input_image.size()\n",
        "\n",
        "    logit = self.model_arch(input_image)\n",
        "    if class_idx is None:\n",
        "        score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "    else:\n",
        "        score = logit[:, class_idx].squeeze()\n",
        "\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "      score = score.cuda()\n",
        "      logit = logit.cuda()\n",
        "\n",
        "    self.model_arch.zero_grad()\n",
        "    score.backward(retain_graph=retain_graph)\n",
        "    gradients = self.gradients['value']\n",
        "    activations = self.activations['value']\n",
        "    b, k, u, v = gradients.size()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      activations = activations.cuda()\n",
        "      gradients = gradients.cuda() \n",
        "\n",
        "\n",
        "    mean = 0\n",
        "    param_n = 35\n",
        "    param_sigma_multiplier = 2\n",
        "\n",
        "    grad_2, grad_3 = torch.zeros_like(activations), torch.zeros_like(activations)\n",
        "\n",
        "    for i in range(param_n):\n",
        "      \n",
        "      noise = Variable(input_image.data.new(input_image.size()).normal_(0,param_sigma_multiplier**2))\n",
        "\n",
        "      noisy_input = input_image + noise\n",
        "      noisy_input = noisy_input.cuda()\n",
        "      out = self.model_arch(noisy_input)\n",
        "      score = out[:, out.max(1)[-1]].squeeze()\n",
        "      self.model_arch.zero_grad()\n",
        "      score.backward(retain_graph=retain_graph)\n",
        "      gradient = self.gradients['value']\n",
        "      \n",
        "      grad_2.add_(gradient.pow(2))\n",
        "      grad_3.add_(gradient.pow(3))\n",
        "\n",
        "    grad_2.div_(param_n)\n",
        "    grad_3.div_(param_n)\n",
        "\n",
        "    # Alpha coefficient for each pixel\n",
        "\n",
        "    global_sum = activations.view(b, k, u * v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "    alpha_num = grad_2\n",
        "    alpha_denom = grad_2.mul(2) + global_sum.mul(grad_3)\n",
        "\n",
        "    alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "\n",
        "    alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "    positive_gradients = F.relu(score.exp() * gradients)\n",
        "    weights = (alpha * positive_gradients).view(b, k, u * v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "    saliency_map = F.relu(saliency_map)\n",
        "    saliency_map = F.interpolate(saliency_map, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "    return saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "      return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "\n",
        "class GradCAMpp(GradCAM):\n",
        "    \"\"\"\n",
        "        GradCAM++, inherit from BaseCAM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super(GradCAMpp, self).__init__(model_dict)\n",
        "\n",
        "    def forward(self, input_image, class_idx=None, retain_graph=False):\n",
        "\n",
        "        \"\"\"Generates GradCAM++ result.\n",
        "        # Arguments\n",
        "            input_image: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "            class_idx: int. Index of target class. Defaults to be index of predicted class.\n",
        "        # Return\n",
        "            Result of GradCAM++ (torch.Tensor) with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        b, c, h, w = input_image.size()\n",
        "\n",
        "        logit = self.model_arch(input_image)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "          score = score.cuda()\n",
        "          logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          activations = activations.cuda()\n",
        "          gradients = gradients.cuda()\n",
        "        \n",
        "        alpha_num = gradients.pow(2)\n",
        "\n",
        "        global_sum = activations.view(b, k, u * v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + global_sum.mul(gradients.pow(3))\n",
        "\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (alpha * positive_gradients).view(b, k, u * v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map\n",
        "\n",
        "\n",
        "class ScoreCAM(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input).cuda()\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          predicted_class= predicted_class.cuda()\n",
        "          score = score.cuda()\n",
        "          logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        \n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          activations = activations.cuda()\n",
        "          score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "              saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "              \n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "              # normalize to 0-1\n",
        "              norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              # how much increase if keeping the highlighted region\n",
        "              # predication on masked input\n",
        "              output = self.model_arch(input * norm_saliency_map)\n",
        "              output = F.softmax(output)\n",
        "              score = output[0][predicted_class]\n",
        "\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "class ScoreCAM2(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input)\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          predicted_class= predicted_class.cuda()\n",
        "          score = score.cuda()\n",
        "          logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b1, k, u, v = activations.size()\n",
        "\n",
        "        #gradients = self.gradients['value']\n",
        "        #b2, k, u, v = gradients.size()\n",
        "\n",
        "        #alpha = gradients.view(b2, k, -1).mean(2)\n",
        "        #weights = alpha.view(b2, k, 1, 1)\n",
        "        #print(weights.size())\n",
        "        \n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          activations = activations.cuda()\n",
        "          score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        mean = 0\n",
        "        param_n = 35\n",
        "        param_sigma_multiplier = 2\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "              saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              #saliency_map *= torch.unsqueeze(weights[:, i, :, :], 1)\n",
        "              \n",
        "              #saliency_map = (saliency_map).sum(1, keepdim=True)\n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        " \n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "\n",
        "                # normalize to 0-1\n",
        "              #saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              #x = input * saliency_map\n",
        "\n",
        "              x = saliency_map               \n",
        "\n",
        "              if (torch.max(x) - torch.min(x)).item() == 0:\n",
        "                continue\n",
        "              else:\n",
        "                sigma = param_sigma_multiplier / (torch.max(x) - torch.min(x)).item()\n",
        "              \n",
        "              score_list = []\n",
        "              noisy_list = []\n",
        "              \n",
        "              for _ in range(param_n):\n",
        "\n",
        "                noise = Variable(x.data.new(x.size()).normal_(mean, sigma**2))\n",
        "                #noise = Variable(x.data.new(x.size()).uniform_(0,1))\n",
        "                noisy_img = x + noise\n",
        "\n",
        "                noisy_list.append(noisy_img)\n",
        "               \n",
        "                output = self.model_arch(noisy_img * input)\n",
        "                output = F.softmax(output)\n",
        "                score = output[0][predicted_class]\n",
        "                score_list.append(score)\n",
        "              \n",
        "              \n",
        "              score = sum(score_list) / len(score_list)\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "class ScoreCAM3(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input)\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          predicted_class= predicted_class.cuda()\n",
        "          score = score.cuda()\n",
        "          logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b1, k, u, v = activations.size()\n",
        "\n",
        "        #gradients = self.gradients['value']\n",
        "        #b2, k, u, v = gradients.size()\n",
        "\n",
        "        #alpha = gradients.view(b2, k, -1).mean(2)\n",
        "        #weights = alpha.view(b2, k, 1, 1)\n",
        "        #print(weights.size())\n",
        "        \n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          activations = activations.cuda()\n",
        "          score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        mean = 0\n",
        "        param_n = 35\n",
        "        param_sigma_multiplier = 2\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "              saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              #saliency_map *= torch.unsqueeze(weights[:, i, :, :], 1)\n",
        "              \n",
        "              #saliency_map = (saliency_map).sum(1, keepdim=True)\n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "\n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "\n",
        "              #if i % 50 == 0 and i < 300:\n",
        "              #  visualize(input.cpu(), saliency_map.type(torch.FloatTensor).cpu())\n",
        "              # normalize to 0-1\n",
        "              norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              # how much increase if keeping the highlighted region\n",
        "              # predication on masked input\n",
        "              #output = self.model_arch(input * norm_saliency_map)\n",
        "              #output = F.softmax(output)\n",
        "              #score = output[0][predicted_class]\n",
        "\n",
        "\n",
        "                # normalize to 0-1\n",
        "              #saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              x = input * norm_saliency_map\n",
        "\n",
        "              #if i % 50 == 0 and i < 300:\n",
        "              #  visualize(input.cpu(), x.type(torch.FloatTensor).cpu())         \n",
        "\n",
        "\n",
        "              if (torch.max(x) - torch.min(x)).item() == 0:\n",
        "                continue\n",
        "              else:\n",
        "                sigma = param_sigma_multiplier / (torch.max(x) - torch.min(x)).item()\n",
        "              \n",
        "              score_list = []\n",
        "              noisy_list = []\n",
        "              \n",
        "              for i in range(param_n):\n",
        "\n",
        "                noise = Variable(x.data.new(x.size()).normal_(mean, sigma**2))\n",
        "                #noise = Variable(x.data.new(x.size()).uniform_(0,1))\n",
        "\n",
        "                noisy_img = x + noise\n",
        "\n",
        "                noisy_list.append(noisy_img)\n",
        "                \n",
        "                #if i % 10 == 0:\n",
        "                #  visualize(input.cpu(), noisy_img.type(torch.FloatTensor).cpu())\n",
        "                \n",
        "                output = self.model_arch(noisy_img)\n",
        "                output = F.softmax(output)\n",
        "                score = output[0][predicted_class]\n",
        "                score_list.append(score)\n",
        "                \n",
        "              score = sum(score_list) / len(score_list)\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCHca4QC-SUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlTmEHVc_3S0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "b44eacba-4c85-4dfe-d41a-16f320ce7a14"
      },
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "#model = model.cuda()\n",
        "model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad0AUy52U3fh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "e2e2652c-86a3-432b-fae5-566f25544caa"
      },
      "source": [
        "n_inputs = model.classifier[6].in_features\n",
        "\n",
        "# Add on classifier\n",
        "model.classifier[6] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 1000), nn.Softmax(dim=1))\n",
        "\n",
        "model.classifier"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "    (1): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DUa-TNNU75R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8hjUwVB-U6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07973c01-5e8e-44b8-f437-426cf3419e76"
      },
      "source": [
        "x = '/gdrive/My Drive/Ins_Del_curve_images/'\n",
        "\n",
        "f = os.listdir(x)\n",
        "\n",
        "new_f = x + f[0]\n",
        "\n",
        "print(new_f)  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Ins_Del_curve_images/gradcam_dog1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcE2N8Rq-1fY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd644ff0-d79c-48f7-bdac-19ed459ce458"
      },
      "source": [
        "img = load_image(new_f)\n",
        "print(img)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PIL.Image.Image image mode=L size=95x95 at 0x7FD2B73DDA90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaBF4f9n_DLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d7e5de2d-acde-4314-99bf-7fdb92250538"
      },
      "source": [
        "img_arr = np.array(img)\n",
        "img_arr"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[35, 34, 33, ..., 43, 44, 44],\n",
              "       [36, 35, 34, ..., 44, 44, 45],\n",
              "       [36, 35, 35, ..., 43, 44, 45],\n",
              "       ...,\n",
              "       [84, 85, 86, ..., 33, 34, 33],\n",
              "       [85, 85, 86, ..., 32, 34, 33],\n",
              "       [86, 85, 85, ..., 31, 31, 31]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Op8Ys4g_J1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3d84a23-43cf-4272-b238-7dff3890945f"
      },
      "source": [
        "print(img_arr.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(95, 95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkQbxi23B0qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#img_arr = np.expand_dims(img_arr, 0)\n",
        "#print(img_arr.shape)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw5b4sOg_2ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb6pxhPOKWtg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37084614-6e6e-4fca-c677-33274584b222"
      },
      "source": [
        "X = model.eval()\n",
        "imx = load_image1(new_f)\n",
        "input_ = apply_transforms(imx)\n",
        "input_ = input_.cuda()\n",
        "X(input_)[0][165].item()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0005423059919849038"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLWxWF0LLHZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6aa4f996-79ac-4683-92e8-ca6803e73d16"
      },
      "source": [
        "X(input_).max(1)[0].item()\n",
        "X(input_).max(1)[-1].item()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96MHyW_JLBfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98de5fb3-c08a-4f39-ca88-5281669985ad"
      },
      "source": [
        "X = model.eval()\n",
        "imx = load_image1(new_f)\n",
        "input_ = apply_transforms(imx)\n",
        "input_ = input_.cuda()\n",
        "print(X(input_)[0])\n",
        "print(len(X(input_)[0]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0005, 0.0013, 0.0003, 0.0010, 0.0007, 0.0005, 0.0009, 0.0008, 0.0007,\n",
            "        0.0009, 0.0014, 0.0014, 0.0007, 0.0002, 0.0004, 0.0010, 0.0009, 0.0008,\n",
            "        0.0013, 0.0020, 0.0006, 0.0004, 0.0011, 0.0004, 0.0006, 0.0004, 0.0013,\n",
            "        0.0005, 0.0012, 0.0005, 0.0009, 0.0007, 0.0004, 0.0006, 0.0018, 0.0016,\n",
            "        0.0019, 0.0008, 0.0024, 0.0015, 0.0012, 0.0013, 0.0007, 0.0006, 0.0006,\n",
            "        0.0008, 0.0005, 0.0006, 0.0018, 0.0006, 0.0029, 0.0039, 0.0017, 0.0008,\n",
            "        0.0003, 0.0003, 0.0013, 0.0012, 0.0007, 0.0009, 0.0005, 0.0005, 0.0004,\n",
            "        0.0016, 0.0012, 0.0005, 0.0008, 0.0007, 0.0007, 0.0008, 0.0008, 0.0006,\n",
            "        0.0012, 0.0030, 0.0021, 0.0006, 0.0011, 0.0008, 0.0004, 0.0007, 0.0006,\n",
            "        0.0013, 0.0010, 0.0011, 0.0003, 0.0008, 0.0015, 0.0005, 0.0020, 0.0010,\n",
            "        0.0011, 0.0014, 0.0012, 0.0007, 0.0006, 0.0028, 0.0004, 0.0004, 0.0007,\n",
            "        0.0019, 0.0007, 0.0006, 0.0011, 0.0016, 0.0003, 0.0005, 0.0023, 0.0005,\n",
            "        0.0011, 0.0010, 0.0004, 0.0005, 0.0013, 0.0008, 0.0006, 0.0008, 0.0012,\n",
            "        0.0006, 0.0012, 0.0007, 0.0010, 0.0009, 0.0038, 0.0004, 0.0007, 0.0018,\n",
            "        0.0006, 0.0005, 0.0020, 0.0006, 0.0021, 0.0032, 0.0043, 0.0011, 0.0008,\n",
            "        0.0031, 0.0015, 0.0006, 0.0004, 0.0010, 0.0003, 0.0011, 0.0005, 0.0010,\n",
            "        0.0025, 0.0011, 0.0008, 0.0006, 0.0008, 0.0007, 0.0017, 0.0008, 0.0005,\n",
            "        0.0005, 0.0024, 0.0004, 0.0015, 0.0006, 0.0011, 0.0014, 0.0015, 0.0004,\n",
            "        0.0009, 0.0012, 0.0006, 0.0005, 0.0005, 0.0012, 0.0014, 0.0011, 0.0006,\n",
            "        0.0007, 0.0010, 0.0009, 0.0009, 0.0018, 0.0026, 0.0013, 0.0006, 0.0005,\n",
            "        0.0011, 0.0009, 0.0008, 0.0006, 0.0009, 0.0009, 0.0008, 0.0009, 0.0003,\n",
            "        0.0010, 0.0012, 0.0008, 0.0008, 0.0004, 0.0013, 0.0017, 0.0038, 0.0006,\n",
            "        0.0005, 0.0009, 0.0012, 0.0006, 0.0008, 0.0007, 0.0008, 0.0006, 0.0015,\n",
            "        0.0012, 0.0012, 0.0010, 0.0016, 0.0007, 0.0004, 0.0003, 0.0007, 0.0010,\n",
            "        0.0008, 0.0006, 0.0033, 0.0013, 0.0005, 0.0009, 0.0011, 0.0017, 0.0008,\n",
            "        0.0006, 0.0010, 0.0008, 0.0004, 0.0009, 0.0012, 0.0022, 0.0016, 0.0010,\n",
            "        0.0004, 0.0005, 0.0011, 0.0003, 0.0008, 0.0015, 0.0005, 0.0015, 0.0005,\n",
            "        0.0008, 0.0015, 0.0004, 0.0006, 0.0006, 0.0010, 0.0006, 0.0005, 0.0005,\n",
            "        0.0014, 0.0004, 0.0013, 0.0013, 0.0016, 0.0006, 0.0002, 0.0010, 0.0012,\n",
            "        0.0007, 0.0004, 0.0008, 0.0009, 0.0010, 0.0013, 0.0005, 0.0009, 0.0009,\n",
            "        0.0014, 0.0020, 0.0011, 0.0012, 0.0018, 0.0007, 0.0004, 0.0007, 0.0027,\n",
            "        0.0007, 0.0008, 0.0012, 0.0023, 0.0002, 0.0006, 0.0011, 0.0013, 0.0010,\n",
            "        0.0009, 0.0003, 0.0018, 0.0008, 0.0002, 0.0009, 0.0037, 0.0007, 0.0006,\n",
            "        0.0005, 0.0004, 0.0010, 0.0009, 0.0006, 0.0004, 0.0014, 0.0008, 0.0006,\n",
            "        0.0005, 0.0007, 0.0009, 0.0004, 0.0008, 0.0017, 0.0012, 0.0016, 0.0008,\n",
            "        0.0004, 0.0017, 0.0010, 0.0006, 0.0012, 0.0007, 0.0034, 0.0007, 0.0013,\n",
            "        0.0016, 0.0012, 0.0021, 0.0005, 0.0024, 0.0004, 0.0009, 0.0004, 0.0016,\n",
            "        0.0011, 0.0008, 0.0011, 0.0002, 0.0005, 0.0010, 0.0017, 0.0010, 0.0004,\n",
            "        0.0004, 0.0005, 0.0008, 0.0008, 0.0007, 0.0014, 0.0003, 0.0009, 0.0032,\n",
            "        0.0006, 0.0011, 0.0022, 0.0010, 0.0005, 0.0017, 0.0009, 0.0021, 0.0008,\n",
            "        0.0013, 0.0013, 0.0010, 0.0002, 0.0013, 0.0002, 0.0007, 0.0011, 0.0017,\n",
            "        0.0009, 0.0010, 0.0006, 0.0012, 0.0006, 0.0028, 0.0014, 0.0009, 0.0007,\n",
            "        0.0011, 0.0011, 0.0006, 0.0011, 0.0009, 0.0008, 0.0007, 0.0009, 0.0009,\n",
            "        0.0008, 0.0004, 0.0003, 0.0007, 0.0006, 0.0010, 0.0009, 0.0015, 0.0004,\n",
            "        0.0020, 0.0027, 0.0005, 0.0010, 0.0008, 0.0004, 0.0005, 0.0006, 0.0006,\n",
            "        0.0005, 0.0007, 0.0007, 0.0006, 0.0009, 0.0012, 0.0017, 0.0017, 0.0007,\n",
            "        0.0012, 0.0005, 0.0010, 0.0016, 0.0008, 0.0004, 0.0037, 0.0009, 0.0003,\n",
            "        0.0007, 0.0004, 0.0004, 0.0015, 0.0010, 0.0012, 0.0005, 0.0007, 0.0013,\n",
            "        0.0012, 0.0018, 0.0006, 0.0006, 0.0043, 0.0026, 0.0005, 0.0010, 0.0002,\n",
            "        0.0011, 0.0006, 0.0010, 0.0006, 0.0017, 0.0008, 0.0003, 0.0021, 0.0016,\n",
            "        0.0010, 0.0005, 0.0011, 0.0011, 0.0011, 0.0006, 0.0011, 0.0018, 0.0006,\n",
            "        0.0010, 0.0011, 0.0011, 0.0012, 0.0005, 0.0009, 0.0006, 0.0005, 0.0007,\n",
            "        0.0005, 0.0007, 0.0011, 0.0017, 0.0004, 0.0003, 0.0014, 0.0018, 0.0009,\n",
            "        0.0010, 0.0010, 0.0009, 0.0004, 0.0027, 0.0004, 0.0016, 0.0005, 0.0008,\n",
            "        0.0008, 0.0010, 0.0007, 0.0025, 0.0016, 0.0013, 0.0009, 0.0009, 0.0019,\n",
            "        0.0011, 0.0006, 0.0012, 0.0009, 0.0007, 0.0007, 0.0007, 0.0019, 0.0007,\n",
            "        0.0014, 0.0002, 0.0019, 0.0012, 0.0010, 0.0005, 0.0009, 0.0007, 0.0008,\n",
            "        0.0006, 0.0005, 0.0020, 0.0011, 0.0004, 0.0002, 0.0008, 0.0007, 0.0013,\n",
            "        0.0003, 0.0006, 0.0007, 0.0012, 0.0010, 0.0012, 0.0005, 0.0006, 0.0013,\n",
            "        0.0008, 0.0031, 0.0009, 0.0009, 0.0006, 0.0006, 0.0007, 0.0010, 0.0014,\n",
            "        0.0007, 0.0013, 0.0017, 0.0005, 0.0007, 0.0026, 0.0004, 0.0007, 0.0008,\n",
            "        0.0008, 0.0008, 0.0009, 0.0008, 0.0018, 0.0015, 0.0008, 0.0019, 0.0018,\n",
            "        0.0008, 0.0008, 0.0007, 0.0005, 0.0005, 0.0008, 0.0009, 0.0006, 0.0005,\n",
            "        0.0005, 0.0005, 0.0003, 0.0004, 0.0005, 0.0008, 0.0011, 0.0014, 0.0014,\n",
            "        0.0012, 0.0011, 0.0009, 0.0019, 0.0010, 0.0011, 0.0006, 0.0012, 0.0023,\n",
            "        0.0010, 0.0013, 0.0018, 0.0006, 0.0011, 0.0006, 0.0010, 0.0032, 0.0009,\n",
            "        0.0013, 0.0012, 0.0016, 0.0007, 0.0023, 0.0008, 0.0016, 0.0006, 0.0003,\n",
            "        0.0018, 0.0007, 0.0008, 0.0008, 0.0022, 0.0011, 0.0017, 0.0006, 0.0016,\n",
            "        0.0005, 0.0005, 0.0015, 0.0005, 0.0004, 0.0007, 0.0013, 0.0009, 0.0026,\n",
            "        0.0009, 0.0009, 0.0007, 0.0009, 0.0015, 0.0006, 0.0004, 0.0010, 0.0006,\n",
            "        0.0014, 0.0009, 0.0010, 0.0004, 0.0008, 0.0008, 0.0007, 0.0030, 0.0006,\n",
            "        0.0005, 0.0005, 0.0027, 0.0007, 0.0006, 0.0004, 0.0007, 0.0003, 0.0012,\n",
            "        0.0013, 0.0007, 0.0007, 0.0003, 0.0009, 0.0008, 0.0002, 0.0017, 0.0010,\n",
            "        0.0008, 0.0015, 0.0012, 0.0008, 0.0006, 0.0009, 0.0004, 0.0009, 0.0017,\n",
            "        0.0004, 0.0003, 0.0012, 0.0004, 0.0007, 0.0005, 0.0012, 0.0004, 0.0006,\n",
            "        0.0012, 0.0011, 0.0024, 0.0005, 0.0031, 0.0005, 0.0005, 0.0011, 0.0015,\n",
            "        0.0004, 0.0006, 0.0009, 0.0011, 0.0011, 0.0016, 0.0011, 0.0013, 0.0006,\n",
            "        0.0008, 0.0025, 0.0008, 0.0015, 0.0011, 0.0008, 0.0010, 0.0005, 0.0011,\n",
            "        0.0004, 0.0011, 0.0007, 0.0004, 0.0007, 0.0021, 0.0007, 0.0006, 0.0013,\n",
            "        0.0010, 0.0006, 0.0010, 0.0011, 0.0002, 0.0004, 0.0009, 0.0020, 0.0005,\n",
            "        0.0009, 0.0003, 0.0010, 0.0011, 0.0020, 0.0006, 0.0006, 0.0007, 0.0006,\n",
            "        0.0009, 0.0006, 0.0009, 0.0023, 0.0007, 0.0013, 0.0007, 0.0012, 0.0017,\n",
            "        0.0013, 0.0002, 0.0014, 0.0004, 0.0010, 0.0013, 0.0005, 0.0007, 0.0014,\n",
            "        0.0010, 0.0014, 0.0012, 0.0016, 0.0003, 0.0018, 0.0007, 0.0018, 0.0006,\n",
            "        0.0005, 0.0013, 0.0007, 0.0013, 0.0008, 0.0012, 0.0005, 0.0006, 0.0012,\n",
            "        0.0015, 0.0004, 0.0017, 0.0037, 0.0028, 0.0009, 0.0007, 0.0006, 0.0005,\n",
            "        0.0012, 0.0006, 0.0006, 0.0005, 0.0005, 0.0006, 0.0009, 0.0012, 0.0007,\n",
            "        0.0004, 0.0014, 0.0009, 0.0014, 0.0010, 0.0009, 0.0008, 0.0011, 0.0006,\n",
            "        0.0004, 0.0013, 0.0004, 0.0008, 0.0006, 0.0019, 0.0013, 0.0011, 0.0005,\n",
            "        0.0010, 0.0009, 0.0008, 0.0015, 0.0006, 0.0005, 0.0007, 0.0008, 0.0008,\n",
            "        0.0005, 0.0010, 0.0005, 0.0017, 0.0018, 0.0003, 0.0006, 0.0022, 0.0006,\n",
            "        0.0012, 0.0008, 0.0016, 0.0005, 0.0018, 0.0003, 0.0009, 0.0004, 0.0012,\n",
            "        0.0010, 0.0003, 0.0006, 0.0007, 0.0007, 0.0020, 0.0008, 0.0002, 0.0014,\n",
            "        0.0004, 0.0004, 0.0004, 0.0012, 0.0011, 0.0009, 0.0009, 0.0004, 0.0010,\n",
            "        0.0013, 0.0007, 0.0015, 0.0015, 0.0006, 0.0008, 0.0010, 0.0010, 0.0021,\n",
            "        0.0004, 0.0007, 0.0006, 0.0008, 0.0012, 0.0005, 0.0015, 0.0003, 0.0009,\n",
            "        0.0013, 0.0007, 0.0009, 0.0005, 0.0013, 0.0011, 0.0008, 0.0008, 0.0009,\n",
            "        0.0016, 0.0010, 0.0009, 0.0013, 0.0013, 0.0006, 0.0022, 0.0007, 0.0004,\n",
            "        0.0006, 0.0006, 0.0006, 0.0019, 0.0010, 0.0007, 0.0006, 0.0012, 0.0006,\n",
            "        0.0016, 0.0011, 0.0007, 0.0005, 0.0009, 0.0011, 0.0018, 0.0016, 0.0011,\n",
            "        0.0008, 0.0008, 0.0007, 0.0006, 0.0026, 0.0006, 0.0008, 0.0016, 0.0011,\n",
            "        0.0008, 0.0011, 0.0003, 0.0018, 0.0011, 0.0012, 0.0007, 0.0020, 0.0005,\n",
            "        0.0017, 0.0008, 0.0007, 0.0010, 0.0006, 0.0004, 0.0011, 0.0011, 0.0007,\n",
            "        0.0008, 0.0011, 0.0002, 0.0010, 0.0006, 0.0010, 0.0008, 0.0003, 0.0010,\n",
            "        0.0007, 0.0010, 0.0006, 0.0006, 0.0005, 0.0003, 0.0005, 0.0009, 0.0007,\n",
            "        0.0016, 0.0009, 0.0007, 0.0026, 0.0017, 0.0006, 0.0008, 0.0022, 0.0005,\n",
            "        0.0002, 0.0007, 0.0011, 0.0004, 0.0005, 0.0018, 0.0005, 0.0011, 0.0007,\n",
            "        0.0007, 0.0009, 0.0010, 0.0012, 0.0007, 0.0013, 0.0008, 0.0015, 0.0007,\n",
            "        0.0017, 0.0005, 0.0004, 0.0014, 0.0009, 0.0005, 0.0008, 0.0007, 0.0006,\n",
            "        0.0011, 0.0014, 0.0006, 0.0006, 0.0011, 0.0024, 0.0003, 0.0003, 0.0009,\n",
            "        0.0014, 0.0007, 0.0011, 0.0012, 0.0012, 0.0023, 0.0012, 0.0011, 0.0010,\n",
            "        0.0006], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADUz2Fho_RcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "6f570fe1-b4c3-42ad-eab2-c5df8a52ed23"
      },
      "source": [
        "c = 0\n",
        "new_img = 0\n",
        "\n",
        "l = []\n",
        "\n",
        "for i in range(len(img_arr)):\n",
        "  for j in range(len(img_arr[0])):\n",
        "\n",
        "    img_arr[i][j] = 0\n",
        "\n",
        "    if c % 10 == 0:\n",
        "      new_img = img_arr\n",
        "      img = Image.fromarray(new_img , 'L')\n",
        "\n",
        "      img.save('./My Drive/_saved_heloo.jpeg')\n",
        "      \n",
        "      im = load_image('./My Drive/_saved_heloo.jpeg')\n",
        "      #print(im.shape)\n",
        "      im = np.array(im)\n",
        "      im = np.expand_dims(im, 0)\n",
        "      im = Image.fromarray(im, 'RGB')\n",
        "      input_ = apply_transforms(im)\n",
        "\n",
        "      input_ = input_.cuda()\n",
        "      l.append(X(input_)[0][165].item())\n",
        "\n",
        "    c += 1\n",
        "\n",
        "print(l)\n",
        "\n",
        "plt.plot(l)\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0007155262865126133, 0.0007155262865126133, 0.0007155262865126133, 0.0007155262865126133, 0.0007134047918953001, 0.0007164716953411698, 0.000708879204466939, 0.000708879204466939, 0.000708879204466939, 0.000708879204466939, 0.000708879204466939, 0.000708879204466939, 0.000708879204466939, 0.000708879204466939, 0.0006881432491354644, 0.0006935059791430831, 0.0006935059791430831, 0.0006935059791430831, 0.0006935059791430831, 0.0006935059791430831, 0.0006935059791430831, 0.0006935059791430831, 0.0006935059791430831, 0.0006893511745147407, 0.0006909260991960764, 0.0006911451346240938, 0.0006911451346240938, 0.0006911451346240938, 0.0006911451346240938, 0.0006911451346240938, 0.0006911451346240938, 0.0006911451346240938, 0.0006911451346240938, 0.0006923455512151122, 0.0006940646562725306, 0.0006940646562725306, 0.0006940646562725306, 0.0006940646562725306, 0.0006940646562725306, 0.0006940646562725306, 0.0006940646562725306, 0.0006940646562725306, 0.0006958653684705496, 0.0006972323171794415, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006960731116123497, 0.0006933576660230756, 0.0006933576660230756, 0.0006933576660230756, 0.0006933576660230756, 0.0006933576660230756, 0.0006933576660230756, 0.0006933576660230756, 0.0006933576660230756, 0.0006970980321057141, 0.0006963522755540907, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.0006927328067831695, 0.000692287809215486, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122, 0.0006923455512151122]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19qxTdznZ6mx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0f812801-5efe-4b95-f20f-677b99eee9d9"
      },
      "source": [
        "for i in range(len(l)):\n",
        "  l[i] = (l[i] - min(l))  / (max(l) - min(l))\n",
        "\n",
        "plt.plot(l)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTGLKRhHFP81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_f = x + f[1]\n",
        "img = load_image(new_f)\n",
        "img_arr = np.array(img)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3OeVXDfLzc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49a327bd-6a12-42b5-8377-ea4ce1e5202b"
      },
      "source": [
        "X = model.eval()\n",
        "imx = load_image1(new_f)\n",
        "input_ = apply_transforms(imx)\n",
        "input_ = input_.cuda()\n",
        "np.exp(X(input_)[0][165].item())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0005937408420702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBicB12WBN-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "415c34f2-f7e9-42b4-fbcb-0bc2c717e66f"
      },
      "source": [
        "c = 0\n",
        "new_img = 0\n",
        "\n",
        "X = model.eval()\n",
        "l1 = []\n",
        "\n",
        "for i in range(len(img_arr)):\n",
        "  for j in range(len(img_arr[0])):\n",
        "\n",
        "    img_arr[i][j] = 0\n",
        "\n",
        "    if c % 50 == 0:\n",
        "      new_img = img_arr\n",
        "      img = Image.fromarray(new_img , 'L')\n",
        "\n",
        "      img.save('./My Drive/_saved_heloo.jpeg')\n",
        "      \n",
        "      im = load_image('./My Drive/_saved_heloo.jpeg')\n",
        "      #print(im.shape)\n",
        "      im = np.array(im)\n",
        "      im = np.expand_dims(im, 0)\n",
        "      im = Image.fromarray(im, 'RGB')\n",
        "      input_ = apply_transforms(im)\n",
        "\n",
        "      input_ = input_.cuda()\n",
        "\n",
        "      l1.append(X(input_)[0][165].item())\n",
        "\n",
        "    c += 1\n",
        "\n",
        "print(l1)\n",
        "\n",
        "plt.plot(l1)\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0009013155940920115, 0.0009084291523322463, 0.0009015434188768268, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000820298446342349, 0.000820298446342349, 0.000820298446342349, 0.000820298446342349, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617, 0.000816353305708617]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImnwGBXRbbca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "ab0cdb76-a963-4fed-a5ed-1d371d230214"
      },
      "source": [
        "for i in range(len(l1)):\n",
        "  l1[i] = (l1[i] - max(l1))  / (max(l1) -min(l1))\n",
        "\n",
        "plt.plot(l)\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtlRKKFAFlVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "147fa406-3e47-44e5-dac0-331278713a80"
      },
      "source": [
        "new_f = x + f[2]\n",
        "img = load_image(new_f)\n",
        "img_arr = np.array(img)\n",
        "\n",
        "c = 0\n",
        "new_img = 0\n",
        "\n",
        "X = model.eval()\n",
        "l2 = []\n",
        "\n",
        "for i in range(len(img_arr)):\n",
        "  for j in range(len(img_arr[0])):\n",
        "\n",
        "    img_arr[i][j] = 0\n",
        "\n",
        "    if c % 50 == 0:\n",
        "      new_img = img_arr\n",
        "      img = Image.fromarray(new_img , 'L')\n",
        "\n",
        "      img.save('./My Drive/_saved_heloo.jpeg')\n",
        "      \n",
        "      im = load_image('./My Drive/_saved_heloo.jpeg')\n",
        "      #print(im.shape)\n",
        "      im = np.array(im)\n",
        "      im = np.expand_dims(im, 0)\n",
        "      im = Image.fromarray(im, 'RGB')\n",
        "      input_ = apply_transforms(im)\n",
        "\n",
        "      input_ = input_.cuda()\n",
        "\n",
        "      l2.append(X(input_)[0][165].item())\n",
        "\n",
        "    c += 1\n",
        "\n",
        "print(l2)\n",
        "\n",
        "plt.plot(l2)\n",
        "plt.show()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[91.93212608806789, 90.2158732060343, 91.85978560708463, 92.33650634996593, 92.33650634996593, 92.62782405130565, 92.62782405130565, 92.60227088816464, 92.60227088816464, 92.42450469173491, 92.42450469173491, 92.62782405130565, 92.62782405130565, 92.50488947145641, 92.50488947145641, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGlHcDLOF4TJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "c0d352a3-1149-4779-f76f-50507037cf77"
      },
      "source": [
        "new_f = x + f[3]\n",
        "img = load_image(new_f)\n",
        "img_arr = np.array(img)\n",
        "\n",
        "c = 0\n",
        "new_img = 0\n",
        "\n",
        "X = model.eval()\n",
        "l3 = []\n",
        "\n",
        "for i in range(len(img_arr)):\n",
        "  for j in range(len(img_arr[0])):\n",
        "\n",
        "    img_arr[i][j] = 0\n",
        "\n",
        "    if c % 50 == 0:\n",
        "      new_img = img_arr\n",
        "      img = Image.fromarray(new_img , 'L')\n",
        "\n",
        "      img.save('./My Drive/_saved_heloo.jpeg')\n",
        "      \n",
        "      im = load_image('./My Drive/_saved_heloo.jpeg')\n",
        "      #print(im.shape)\n",
        "      im = np.array(im)\n",
        "      im = np.expand_dims(im, 0)\n",
        "      im = Image.fromarray(im, 'RGB')\n",
        "      input_ = apply_transforms(im)\n",
        "\n",
        "      input_ = input_.cuda()\n",
        "\n",
        "      l3.append(X(input_)[0][165].item())\n",
        "\n",
        "    c += 1\n",
        "\n",
        "print(l3)\n",
        "\n",
        "plt.plot(l3)\n",
        "plt.show()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[81.67202468030155, 81.83849859051406, 82.34690758399665, 92.62782405130565, 92.62782405130565, 92.35739707946777, 92.35739707946777, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIVnAx1tGDJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "0343dff2-0af5-4070-9fc3-f7dfe66b5c07"
      },
      "source": [
        "new_f = x + f[4]\n",
        "img = load_image(new_f)\n",
        "img_arr = np.array(img)\n",
        "\n",
        "c = 0\n",
        "new_img = 0\n",
        "\n",
        "X = model.eval()\n",
        "l4 = []\n",
        "\n",
        "for i in range(len(img_arr)):\n",
        "  for j in range(len(img_arr[0])):\n",
        "\n",
        "    img_arr[i][j] = 0\n",
        "\n",
        "    if c % 50 == 0:\n",
        "      new_img = img_arr\n",
        "      img = Image.fromarray(new_img , 'L')\n",
        "\n",
        "      img.save('./My Drive/_saved_heloo.jpeg')\n",
        "      \n",
        "      im = load_image('./My Drive/_saved_heloo.jpeg')\n",
        "      #print(im.shape)\n",
        "      im = np.array(im)\n",
        "      im = np.expand_dims(im, 0)\n",
        "      im = Image.fromarray(im, 'RGB')\n",
        "      input_ = apply_transforms(im)\n",
        "\n",
        "      input_ = input_.cuda()\n",
        "\n",
        "      l4.append(X(input_)[0][165].item())\n",
        "\n",
        "    c += 1\n",
        "\n",
        "print(l4)\n",
        "\n",
        "plt.plot(l4)\n",
        "plt.show()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.29227434843779, 92.29227434843779, 92.29227434843779, 92.29227434843779, 94.31748767383397, 89.8513593710959, 92.45945257134736, 92.45945257134736, 92.45945257134736, 92.45945257134736, 92.45945257134736, 92.45945257134736, 92.45945257134736, 92.45945257134736, 86.34657715447247, 92.35739707946777, 92.35739707946777, 92.35739707946777, 92.35739707946777, 92.35739707946777, 92.35739707946777, 92.35739707946777, 92.35739707946777, 91.55386360362172, 91.67835232801735, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 90.16633848659694, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 90.0853076018393, 91.55095904134214, 91.99025225825608, 91.99025225825608, 91.99025225825608, 91.99025225825608, 91.99025225825608, 91.99025225825608, 91.99025225825608, 91.99025225825608, 91.03877819143236, 92.74301119148731, 92.74301119148731, 92.74301119148731, 92.74301119148731, 92.74301119148731, 92.74301119148731, 92.74301119148731, 92.74301119148731, 89.58593825809658, 91.96275495924056, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 89.96218093670905, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWd6PBBAZCGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd4145f8-f7d8-44eb-a750-c5e66e16c417"
      },
      "source": [
        "for i in range(len(l4)):\n",
        "  l4[i] = (l4[i] - 92.29227434843779)  / (94.31748767383397 - 92.29227434843779)\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " -1.2052631427676337,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " 0.08254845097706716,\n",
              " -2.9358374840843964,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " 0.032155985847687314,\n",
              " -0.36460887135018927,\n",
              " -0.30313943362007767,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " 0.1530685858321669,\n",
              " -1.0497342848684645,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " -1.0897453216029707,\n",
              " -0.3660430720060702,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.14913100086511905,\n",
              " -0.6189452445757612,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " 0.2225626492761454,\n",
              " -1.3363214908788863,\n",
              " -0.16270848362740675,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " -1.1505422083241121,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602,\n",
              " 0.16568610262438602]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk0DFBnJGKFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "6e165277-7e84-44a0-f9da-2f05009f963c"
      },
      "source": [
        "new_f = x + f[5]\n",
        "img = load_image(new_f)\n",
        "img_arr = np.array(img)\n",
        "\n",
        "c = 0\n",
        "new_img = 0\n",
        "\n",
        "X = model.eval()\n",
        "l5 = []\n",
        "\n",
        "for i in range(len(img_arr)):\n",
        "  for j in range(len(img_arr[0])):\n",
        "\n",
        "    img_arr[i][j] = 0\n",
        "\n",
        "    if c % 50 == 0:\n",
        "      new_img = img_arr\n",
        "      img = Image.fromarray(new_img , 'L')\n",
        "\n",
        "      img.save('./My Drive/_saved_heloo.jpeg')\n",
        "      \n",
        "      im = load_image('./My Drive/_saved_heloo.jpeg')\n",
        "      #print(im.shape)\n",
        "      im = np.array(im)\n",
        "      im = np.expand_dims(im, 0)\n",
        "      im = Image.fromarray(im, 'RGB')\n",
        "      input_ = apply_transforms(im)\n",
        "\n",
        "      input_ = input_.cuda()\n",
        "\n",
        "      l5.append(X(input_)[0][165].item())\n",
        "\n",
        "    c += 1\n",
        "\n",
        "print(l5)\n",
        "\n",
        "plt.plot(l5)\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.54096657969058, 92.54096657969058, 92.10435673594475, 91.92396537400782, 91.92396537400782, 91.92396537400782, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.35739707946777, 92.35739707946777, 92.35739707946777, 91.71633282676339, 92.62782405130565, 92.62782405130565, 92.62782405130565, 91.54959116131067, 91.9636688195169, 91.9636688195169, 91.9636688195169, 92.22548687830567, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.60227088816464, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565, 92.62782405130565]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM7DUCRbGcRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8be1daa-f783-4c0a-e2e2-1d0ae13cdbb2"
      },
      "source": [
        "len(f)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF4DV5N7Go3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "99e883b0-1f91-4faf-e9b2-4cd44ee21430"
      },
      "source": [
        "plt.plot(l, label='Grad-CAM')\n",
        "plt.plot(l1, label='Grad-CAM++')\n",
        "plt.plot(l2, label='Smooth Grad-CAM++')\n",
        "plt.plot(l3, label='Score-CAM')\n",
        "plt.plot(l4, label='SS-CAM(1)')\n",
        "plt.plot(l5, label='SS-CAM(2)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI666jARGzwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}